DOI: 10.1111/cgf.12527

COMPUTER GRAPHICS

forum

Volume 34 (2015), number 6 pp. 148–160

Saliency-Preserving Slicing Optimization for Effective 3D Printing
Weiming Wang1 , Haiyuan Chao2 , Jing Tong2 , Zhouwang Yang3 , Xin Tong4 , Hang Li3 , Xiuping Liu1 and Ligang Liu3

University of Technology, Dalian, China wwmdlut@gmail.com, xpliu@dlut.edu.cn 2 Hohai University, Changzhou, China chaohaiyuan@gmail.com, Tongjing.cn@gmail.com 3 University of Sciences and Technology of China, Hefei, China yangzw@ustc.edu.cn, lihang49@mail.ustc.edu.cn, lgliu@ustc.edu.cn 4 Microsoft Research Asia, Beijing, China xtong.gfx@gmail.com

1 Dalian

Abstract We present an adaptive slicing scheme for reducing the manufacturing time for 3D printing systems. Based on a new saliencybased metric, our method optimizes the thicknesses of slicing layers to save printing time and preserve the visual quality of the printing results. We formulate the problem as a constrained 0 optimization and compute the slicing result via a two-step optimization scheme. To further reduce printing time, we develop a saliency-based segmentation scheme to partition an object into subparts and then optimize the slicing of each subpart separately. We validate our method with a large set of 3D shapes ranging from CAD models to scanned objects. Results show that our method saves printing time by 30–40% and generates 3D objects that are visually similar to the ones printed with the ﬁnest resolution possible. Keywords: 3D printing, mesh saliency, adaptive slicing, segmentation, visual quality ACM CCS: I.3.5 [Computer Graphics]: Computer Graphics—Computational Geometry and Object Modelling

1. Introduction In recent years, 3D printing has gained considerable attention from graphics researchers due to its relatively low cost and high capability for fabricating 3D objects with arbitrarily complex geometric shapes or topology [DSDB06]. A set of techniques have been developed for manufacturing real 3D objects with various sizes, dynamics, appearances, as well as physical properties [SVB*12, LBRM12, PWLSH13, WWY*13]. Despite these technical advances, the manufacturing speed of 3D printers is still very slow, limiting the usage of 3D printers in many applications. Most Fused Deposition Modelling (FDM) 3D printers fabricate a 3D solid object by accumulating its volume layer by layer. To achieve the best quality result, an input object is sliced uniformly with the ﬁnest resolution of printer hardware and thus results in a large number of layers. As a result, an FDM printer always takes couple of hours to fabricate a small-sized object with a height of 10 cm.

A naive solution for improving printing speed is to increase the thickness of all layers. Although this method reduces the number of layers of input 3D shapes, the discontinuous boundary between the layers greatly decreases the quality of the results. Other methods slice input shapes adaptively by minimizing the local geometry error [PRD03]. Although these approaches work well for CAD models with sparse sharp features, they are not capable of handling models with big salient regions that are popular in current 3D printing application. Moreover, the local and heuristic optimization schemes adopted in these methods only achieve suboptimal results. In this paper, we present an adaptive slicing method for reducing the manufacturing time of 3D printers while preserving the visual quality of printing results. The key observation of our method is that human beings are more sensitive to geometric errors in regions with high visual saliency. We thus propose a saliency-based geometric error metric for evaluating the visual quality of the printing results. Based on this metric, our method slices an input object with the

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.

148

W. Wang et al. / Saliency-Based Slicing Optimization

149

Figure 1: Given the 3D Buddha head model (a), the object printed using our slicing algorithm (b) has indistinguishable visual quality with the ﬁnest one printed by the highest resolution and saves printing time by 29.44%. By applying the segmentation method, the segmented object (c) is printed and saves printing time by 39.11% while also preserving visual appearance (d). All of the savings are relative to the objects printed with the ﬁnest resolution possible. least number of layers that can still preserve the visual quality of the input after printing. We formulate this problem as a constrained 0 optimization and solve the thicknesses of slicing layers with two-step optimization. The ﬁrst time optimization step is for eliminating redundant layers and the second visual optimization step is for minimizing the visual degradation of the printing results. We perform these two steps iteratively to reduce the number of layers until the visual quality degradation of the printing result achieves a user-speciﬁed threshold. To further reduce printing time, we also present a saliency-based segmentation method for partitioning input object into subparts, each of which is sliced independently with our adaptive slicing scheme. In the ﬁnal fabrication, each subpart is printed with its own slicing scheme and all subparts are automatically merged together to avoid the apparent cutting of seams. Figure 1 illustrates a result of our segmentation method. We have tested our method with a large set of 3D objects including both CAD models and scanned 3D objects with rich surface details. We also evaluate printing results generated by our method qualitatively and quantitatively. Results show that our saliency-based metric (SM) works well for both kinds of inputs and the new optimization scheme generates better results than other existing local optimization schemes. Compared to the uniform slicing method with the ﬁnest resolution, our method saves about printing time by 30–40% and generates visually similar results. The contributions of this paper are as follows: 2. Related Work 3D Printing. A set of methods have been developed to extend the capability of current printing hardware for fabricating objects that are large in size [HFW11, LBRM12, VGB*14], have various appearances [VWRKM13, LDPT13, LGX*13] and dynamics [BBJP12, ZXS*12, CCA*12, CTN*13, CLM*13]. Other methods optimize the current 3D printing system by enhancing results structure [SVB*12], [ZPZ13] and stability [PWLSH13], reducing material cost [WWY*13] and automating software pipeline [CLD*13]. Different from all these works, our work focuses on reducing printing time in 3D printing. Adaptive Slicing. Several adaptive slicing methods have been designed for manufacturing CAD models with variant layer thickness [PRD03], in which the result quality is measured by geometric errors (e.g. cusp height, chord length, volumetric deviation, etc.). Most methods [DM94, JH95, TB98, HA13] optimize the slicing of input object with single geometric error deﬁned over whole object surface. Mani et al. [MKD99] present a region-based adaptive slicing scheme that allows different surface regions to have amounts of different geometric errors. However, it is difﬁcult for users to manually specify the acceptable geometric error over the object surface. Since these geometry-based methods do not take surface salience in consideration, they cannot guarantee the visual quality of the printed results. Moreover, all these methods optimize the thickness of slicing layers in a greedy or heuristic way and thus only generate suboptimal results. Contrary to this, our method adapts the scheme in [HA13] for initialization and reﬁnes the results with a new constrained 0 optimization. Compared to geometric-based adaptive slicing solutions, our method successfully preserves the visual quality of the printing results and saves more printing time. Mesh Saliency. Inspired by image saliency works [IKN98, CZM*11], the mesh saliency metrics [SLM*14, LVJ05, CSPF12, WSZL13] measure the importance of points or regions of a 3D surface mesh in a way that is similar to human visual perception.

r r r

A visual saliency metric for guiding adaptive slicing, which works well for both CAD models and 3D shapes with rich surface details. A constrained global 0 optimization for solving the adaptive slicing of input objects. A saliency-based segmentation scheme for further reducing the printing time.

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

150

W. Wang et al. / Saliency-Based Slicing Optimization

Figure 2: Slicing a model into a set of layers colour-coded by layer thickness. (a) Uniform slicing of the input model with minimal layer thickness α; (b) Adaptive slicing of the input model; (c) Uniform slicing of the input model with the maximal layer thickness α. It has been proven that these perception-inspired metrics outperform pure geometry-based metric for a set of mesh manipulation applications [LVJ05]. In our method, we propose an SM for evaluating the visual quality of the printing results and use it to guide the slicing optimization and mesh segmentation [SLM*14]. 3. Problem Formulation Our Goal. Given an input 3D model G with height H along the Z-direction, a slicing scheme cuts G with a set of planes that are parallel to the XY plane to generate a sliced mesh M along Z-direction for 3D printing. As shown in Figure 2, slicing G uniformly with the minimal printable layer thickness α speciﬁed by the 3D printer hardware results in the ﬁnest results M with Nα = H /α layers, which has the longest printing time. On the contrary, slicing G uniformly with the maximal printable layer thickness α leads to the coarsest result with Nα = H /α layers, which takes the shortest printing time. We denote the layer thickness vectors of the ﬁnest and the coarsest results as h and h, respectively. As shown in Figure 2(b), our adaptive slicing method generates a sliced mesh with N layers {L1 , L2 , · · · , LN }, where each layer thickness hi ∈ [α, α] and N hi = H . Our goal is to slice the i=1 input model with optimal layer thickness h = (h1 , h2 , · · · , hN ) so that the result takes the shortest printing time possible and preserves visual quality. Printing Time. Although there are many factors that would affect the printing time of a sliced mesh M [AAD98], we ﬁnd that the printing time of a layer is almost constant no matter how the layer thickness and cutting area may vary because the sum of the travel time and retracting time is larger than the wire squeezing time in most cases. Furthermore, the printer hardware-related time cannot be optimized by us and the print path optimization algorithm is already very mature, so the major factor affecting printing time that can be optimized is the number of layers. As a result, we determine the printing time T (M) of a sliced mesh M by the number of layers T (M) = N under the slicing orientation is ﬁxed which can be optimized in Section 6. Unlike existing solutions that directly reduce the number of layers with a heuristic layer merging or splitting scheme, we set the number of layers of a sliced mesh M as the ﬁnest result Nα = H /α and degenerate the thicknesses of redundant layers as zero and optimize the slicing positions of the rest of the layers. In particular, hi ∈ [α, α]

Figure 3: Saliency maps of the Venus head model. Regions around the face with greater saliency are more visually important than the other regions. for the valid layers and hi = 0 for the degenerated layers. As a result, the printing time of the sliced mesh T (M) can be deﬁned by the 0 -norm (the number of non-zero elements) of h, i.e. h 0 , where h = (h1 , h2 , · · · , hN ). Visual Quality of Printing Result. To evaluate the visual quality of a sliced mesh, we ﬁrst measure the visual importance of the surface points over the mesh and then deﬁne the SM for measuring the visual quality of the printing results. Finally, the visual degradation of the sliced mesh generated by our method is computed by the difference between its visual quality and the one of the ﬁnest result. We apply the method in [SLM*14] to compute the saliency S of original input model G. As shown in Figure 3 (left), the result is a scalar measure of visual importance ∈ [0.0, 1.0] for each point on G, where 0.0 represents the points with the lowest visual importance and 1.0 refers to the highest visual importance. After that, we deﬁne the saliency Si of ith layer Li in the result sliced mesh M of G as the largest saliency value of all surface points in the layer. Based on mesh saliency, we measure the visual quality of the ith layer Li by an SM SM(Li ) = Si · hi · cos θi , (1)

where hi · cos θi is the maximal cusp height of the layer Li . θi = minp∈Li θ (p) is the minimum of the angles θ (p) computed on all surface points in the layer, in which θ (p) is the angle between the norm of a surface point p and the Z-direction. We sum the visual quality metric of all layers to compute the visual quality of the sliced mesh
N

SM(M) = SM(h) =
i=1

SM(Li ).

(2)

Finally, we regard the ﬁnest result M as the ground truth and compute the visual quality degradation of our result mesh M as the SM difference of two meshes E(h) = SM(h) − SM(h). (3)

Note that the visual quality degradation of a slicing result E(h) is between 0.0 and E(h).

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

W. Wang et al. / Saliency-Based Slicing Optimization

151

Figure 4: Overview of our adaptive slicing algorithm. Given an input model (a), our algorithm initializes the slicing result (b). Then our algorithm iteratively executes timing optimization (c) and visual optimization (d) until the visual degradation of the sliced mesh exceeds the user-speciﬁed threshold. The ﬁnal slicing result is (e). Slicing Optimization. Based on printing time and the visual quality metric deﬁned above, we compute an optimal slicing solution h via a constrained 0 -norm optimization min
h

that, we merge the layers according to their merge costs in the ascent order until the total merge costs exceed the user given threshold ε. Timing Optimization. After initialization, we perform timing optimization (TimeOpt for short) to degenerate some layers to save printing time. To this end, we relax semi-continuous interval constraint in Equation (4) by a continuous one hi ∈ [0, α], (5)

h

0

s.t.

hi ∈ {0} ∪ [α, α], i = 1, · · · , N
N i=1

(4)

hi = H,

E(h) < ε, where h 0 determines the printing time, and E(h) computes visual quality degradation of the printing result as Equation (3). ε ∈ [0.0, E(h)] is a threshold speciﬁed by the user to control the visual quality of the printing result. 4. Adaptive Slicing (AdapSlice) The constrained optimization problem (4) is NP-hard because the ﬁrst constraint is a semi-continuous interval constraint so that formulation (4) is a mixed integer programming problem which is NPhard. We solve the problem with a two-step optimization as shown in Figure 4. After initialization, we ﬁrst apply a timing optimization to degenerate the redundant layers. Then a visual optimization step is executed to adjust layer thickness and minimize the visual degradation of the sliced mesh. We repeat these two steps iteratively until the SM difference E(h) exceeds the given threshold ε. Initialization. Starting from h = h, we adopt the greedy scheme in [HA13] to merge layers for initialization. To this end, we ﬁrst compute the merge cost E(Li ∪ Li+1 ) for each layer Li as the SM difference between the original two layers Li and Li+1 and the merged one. If the total thickness of Li and Li+1 exceeds the maximal printable layer thickness hi + hi+1 > α, we assign a large enough merge cost to this layer so that it will not be merged. After

where hi = 0 means that the slicing layer Li is degenerated. Similar to the method in [WWY*13], we solve the 0 -norm optimization in Equation (4) by reformulating it as a weighted 1 optimization min
h

Wh

1

s.t.

hi ∈ [0, α], i = 1, · · · , N
N i=1

(6)

hi = H,

E(h) < ε, where W is a diagonal matrix and w = diag(W) is a favourable weight vector designed for counteracting the inﬂuence of the slicing thickness magnitude on the 1 -norm objective. Based on the solution h from the initialization or the visual optimization, a desired weight vector can be constructed by wi = 1 , η + (hi − α) (7)

where a small number η = 10−8 is set to provide numerical stability. We solve this timing optimization with the interior-point algorithm described in [NW06]. After this optimization, the thicknesses of some layers is less than α and thus can be removed. For this purpose, we collect all

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

152

W. Wang et al. / Saliency-Based Slicing Optimization

Figure 5: Voids (marked in yellow dotted circles) and overlaps (marked in black dotted circles) exist if the cutting boundary is not vertical and the two subparts are sliced independently, as shown in (a) and (b). Thus we allow only vertical planes to segment the object in order to avoid them (c). redundant layers whose thicknesses are below α and compute their merging costs ELi = min(E(Li+1 ∪ Li ), E(Li−1 ∪ Li )). (8)

Figure 6: The segmentation of the Venus head model. Images (a) and (b) show the steps of SegOpt. (a) The model is split into two subparts by a vertical plane and each subpart is sliced independently. The critical points of each subparts partition the model into multiple layer intervals. (b) Some layer intervals are merged (shown in green) and each subpart is independently sliced. Part (c) shows the printed object. The regions in black rectangles are zoomed in and the slicing and artefact can be seen clearly.

We also record the corresponding merging neighbour layer of each redundant layer Li as Li+1 if E(Li+1 ∪ Li ) < E(Li−1 ∪ Li ) or Li−1 otherwise. After that, we degenerate these layers according to the ascendent order of their merge costs until the SM difference of the merged slicing mesh exceeds the user-given threshold ε. In each step, we set the thickness of the current redundant layer to zero and update the thickness of the corresponding merging neighbour layer as the sum of two layers. Visual Optimization. After timing optimization, we obtain the updated thickness vector h. In visual optimization (VisualOpt for short) step, we ﬁx degenerated layers unchanged and reﬁne the thickness of non-zero layers to minimize the total SM difference of sliced meshes. To this end, we have min
h

However, the algorithm is not designed for 3D printing. We thus present a saliency-based segmentation algorithm by considering the geometric constrains posed by 3D printing hardware. Printing Hardware Constraints. Instead of printing subparts individually and then assembling and gluing them into an object, we want to print all subparts of an input model as a whole (as shown in Figures 1d and 6c). Since the subparts with the same cutting boundary are sliced independent, their slicing plane positions might not align along a cutting boundary, as shown in Figures 5(a) and (b). As a result, some voids or overlaps may occur along the cutting boundaries during printing. We have found that the overlaps may seriously damage the printer nozzle. In order to avoid this, we use either horizontal or vertical cutting planes to segment the input model in our solution. Although the horizontal cutting boundaries can be printed well (shown in Figure 5c), the vertical cutting plane still leads to some visual artefacts (shown in Figure 6c) when the object is printed with an FDM printer. Unfortunately, these issues cannot be totally avoided due to the limitations of current FDM printing hardware and the physical properties of the printing materials (we have tested both PolyLactic Acid (PLA) and Acrylonitrile Butadiene Styrene (ABS) materials). Therefore, our segmentation method tries to reduce the number of vertical cutting boundaries as much as possible in order to minimize visual issues in the printed results. Algorithm Overview. With the constraints described above, our method segments the input model progressively. As shown in Algorithm 1, we ﬁrst ﬁnd an optimal vertical plane that can segment the input model into two parts. If the reduced printing time of the two subparts is less than a user-speciﬁed threshold τ (0.85 in our current implementation), our method stops to segment the input model. Otherwise, we cut the input model with several horizontal planes again and then merge subparts between the two neighbouring horizontal planes and thus remove the vertical cutting boundary between two subparts. After that, we execute the adaptive slicing scheme for result subparts again. This algorithm can be executed recursively for each result subpart when necessary. In our implementation, we execute this segmentation algorithm only once for all the input models shown in the paper.

E(h ) hi ∈ [α, α], i = 1, · · · , N
N i=1

s.t.

(9)

hi = H,

where h is the thickness vector of all non-zero layers and N is the number of non-zero layers. We also apply the interior-point algorithm [NW06] to solve the visual optimization. 5. Saliency-Based Segmentation Given a 3D model, the adaptive slicing algorithm (Section 4) can be applied on the whole model and obtain an adaptive slicing solution. However, for some models, different regions at the same slice might have much different saliency. For example, the back region of the Venus head model, as shown in Figure 3 (right), is less salient than the front face part of the model (Figure 3, left). Thus it is not efﬁcient to slice the back region with the same high resolution as the face part. To further reduce printing time, we segment the object into several subparts so that each subpart can be sliced separately with our slicing optimization method. Existing saliency-based segmentation methods (e.g. [SLM*14]) could segment the input model into regions with different saliency.

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

W. Wang et al. / Saliency-Based Slicing Optimization

153

Algorithm 1. SegOpt Input: A 3D model G and T (M) obtained by AdapSlice Output: Its segmentation and the slicing results 1: Find an optimal vertical cutting plane which partitions G into two subparts G1 and G2 2: Call AdapSlice for G1 and G2 separately to obtain M1 , M2 , T (M1 ) and T (M2 ) if ( (T (M1 ) + T (M2 ))/T (M) < τ ) { 3: Horizontal Segmentation and Merging; 4: Call SegOpt for each subpart; } Vertical Cutting Plane Optimization. To ﬁnd the optimal vertical cutting plane for segmentation, we parametrize the cutting plane with a 2D point in the X − Y plane that it passes and the 1D distance of the origin point to the plane. We then generate all possible cutting planes by uniformly sampling parameter space (100 × 100 2D points on a unit circle in the X − Y plane and 50 1D distance samples on the diagonal of the bounding box of the model). For each cutting plane, we trace its intersection seam C with the input model surface and compute the cost function of C as

caused by the vertical segmentation and preserves the printing time of the input model. 6. Model Orientation Both our slicing optimization and segmentation algorithms assume that the orientation of the input model is speciﬁed by user. If this is not the case, we adapt the method in [HBA13] to compute the object orientation that can save printing time and preserve quality. In particular, we follow the normal clustering scheme in [HBA13] to ﬁnd three orthogonal candidate directions. Then we choose orientation to follow the candidate direction with the biggest orientation score, computed by OS (M) = a1 .T imeS (M) + a2 .V olS (M) + a3 .StaS (M), (11) where T imeS , V olS and StaS are printing time score, volumetric error score and printing stability score, respectively. a1 , a2 and a3 are their weights (0.3 in our implementation). The printing stability score is deﬁned by StaS (M) = ATA(M) [XWL*97], where AT (.) G indicates the contact area between the object and the printing plate, AG is the surface area of G. The printing time score is deﬁned T imeS (M) = MAXT − Ti , MAXT − MI NT (12)

f (C) =

C

(Q + λ1 S)dC − λ2 D,

(10)

where Q is the mean curvature of each seam point that is used to encourage the cutting seam C to locate on invisible region [LBRM12]. S is the visual importance of each seam point that is used to constrain the seam to go through the non-salient regions. D measures the difference of the averaged visual importance of two subparts partitioned by C. Large D leads to less overall printing time as the subparts with small averaged visual importance may be sliced thicker and thus take less printing time after segmentation. λ1 and λ2 are the weights (λ1 = 0.3 and λ2 = 0.4). After computation, we choose the cutting plane with the minimal f (C) as the optimal vertical cutting plane. Horizontal Segmentation and Merging. In order to reduce the artefacts on the vertical cutting seams, we try to divide the two subparts with horizontal planes and merge two subparts in each horizontal partition so that the vertical cutting boundary between the two subparts can be removed. To this end, we ﬁnd a set of critical points [ZC95] along the Z-direction for each subpart (see Figure 6b), at which the neighbouring layer thickness abruptly changes (>0.05 in our implementation). These critical points split the object into a set of intervals along the Z-direction. If the average SMs of two subparts within an interval are similar (<0.03 in our implementation), we segment the object with the two horizontal planes of this interval and then merge the two subparts within the interval and remove the vertical cut between them (as shown in Figure 6c). By merging the subparts with similar SM errors, our method reduces the artefacts

where Ti is the printing time of the object in the current orientation. MAXT and MI NT are the maximal and the minimal printing time for objects in all three orientations. Different from [XWL*97] that uses the number of slicing layers of the input object as the printing time, we compute the printing time for both the input object and supporting parts. For this purpose, we send the sliced mesh generated by our method to Cura Engine [Cur13] to generate the printing path for both the input object and the supporting part. We then compute the printing time based on the printing speed of the 3D printer. The new volumetric error score V olS is computed by V olS (M) = 1 − |
N k=1

hk .Pk − VG | , VG

(13)

where Pi is the perimeter of the ith layer, VG is the volume of the input model G. Figure 7 shows an example of the best slicing orientation selection. The corresponding orientation scores of the three candidate directions shown in Figures 7(b–d) are 0.5503, 0.2972 and 0.6927, respectively. As a result, the orientation in Figure 7(d) is chosen as the best slicing orientation. 7. Results Implementation Details. We implement our algorithm in C++ on a PC with Intel(R) Core(Tm) i7-3770K CPU @ 3.50 GHz and 8 GB memory. The results are fabricated by a MakerBot ReplicatorTM 2X, which is an FDM 3D printer with tray size 225 mm × 145 mm × 150 mm [Mak12]. The printable layer thickness of the printer

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

154

W. Wang et al. / Saliency-Based Slicing Optimization

0.5 0.4 0.3 0.2

Figure 7: Slicing orientation selection. Three orthogonal orientations are determined by [HBA13] (a), and the model is sliced along x, y and z orientations which are shown in (b), (c) and (d), respectively.

0.1 0

ranges from α = 0.1 mm to α = 0.4 mm. In all experiments, we scale the input model uniformly so that its height is 80mm. After optimization, the number of layers of the result sliced mesh varies from Nα = 200 to Nα = 800. All objects shown in this paper are printed as a shell with a thickness of 1.2 mm. If a model needs to be printed as solid, we follow the method in [Sab96] to decompose the model into an interior part and an exterior part. The exterior part is sliced by our adaptive slicing algorithm, while the interior part is sliced by the coarsest resolution α. Finally, we use open source slicing engine Cura [Cur13] to convert the resulting sliced mesh into the machine code (G-code) so that it can be fabricated by a 3D printer. We have managed to customize the Cura engine to generate a modiﬁed G-code so that we can generate layers with different levels thickness and print the subparts of an input model as a whole. The slicing paths of each part of the model are generated by Section 5 and all these paths must be assembled together to form one G-code so that the printers can print them together. First, the slicing paths of each part are generated independently. The slicing paths of all parts are then sorted according to ascending order of the slicing height. Finally, the slicing paths are written into the ﬁle one by one according to the order. The generated G-code has been tested on FDM printers successfully (see Figures 1d and 6c). It is worth noting that the difference between two adjacent slicing heights cannot exceed the maximum printable layer thickness. Supporting structures are used to keep the balance of the object during printing which is no longer useful after the object is printed completely, so the printing accuracy of the supporting structures does not affect the accuracy of the whole object. Therefore, the slicing thickness and slicing positions of the supporting structures do not need to be optimized which can be the same as the model. Parameter. The parameter ε ∈ [0, E(h)] used in our algorithm balances between the printing time and the visual quality of the printed object. Denote ε = ρE(h). We allow the user to adjust ρ ∈ [0, 1] to control the result. A smaller value of ρ leads to results with better visual quality and longer printing time (more layers), while a larger value of ρ leads to results with worse visual quality and shorter printing time (less layers), as shown in Figure 9. To guarantee the visual quality of printed objects, we prefer a small value of ρ (we set ρ = 0.05 in our experiments). Convergence. Our algorithm stops when the SM difference of the VisualOpt exceeds the given tolerance. In general, it takes only one to two iterations to converge in AdapSlice and each iteration

Figure 8: The statistics of averaged saving time and variances on testing SegOpt on a data set of 267 models. A total of 80 models is not segmented as the partition does not lead to signiﬁcant saving in time. The others are segmented into multiple subparts, which saves more time during printing.

needs about 100 s to reach the local minimum. Figure 10 only shows the changes of the number of layers and the SM difference of the initialization and ﬁrst two iterations, since the SM difference of the VisualOpt of the third iteration exceeds the user’s given SM difference tolerance. The number of layers is guaranteed to decrease during the iterations (Figure 10, left panel). Each iteration is shown in the skyblue dotted box. The green line shown in Figure 10 (right panel) is the SM difference tolerance given by user. The blue dots in Figure 10 denote the number of layers and the SM difference after corresponding operations, respectively. At the initialization, the SM difference will exceed the given tolerance when a new layer is removed, so the heuristic merging in the initialization cannot remove more layers (that is to say, it cannot reduce printing time further) under the current given SM difference tolerance. However, the proposed AdapSlice can reduce the total SM difference further (See Figure 10, right panel) because TimingOpt can adjust the slicing positions of layers during optimization. Algorithm Performance. We test our slice optimization algorithm and segmentation algorithm with 267 3D models randomly selected from the SHREC database [SMKF04] and the Princeton database [LGB*11]. Among all tested models, 80 3D models are not partitioned because the segmentation does not lead to signiﬁcant savings in printing time. All others are partitioned into three or more subparts after segmentation. Figure 8 illustrates the statistics of the reduced printing time of all results generated by our method. Compared to the ﬁnest results, the results generated by slicing optimization save 26.78% printing time on average compared. For results generated by both slicing optimization and segmentation, they save 34.23% printing time on average compared to the ﬁnest results. Figure 1(c) shows a model partitioned into three subparts using SegOpt and the photo of the printed object is shown in Figure 1(d). The models with large saliency variations over the model (like the Venus head model in Figure 3) can gain more beneﬁts from our segmentation and slicing methods. Method Validation. We evaluate printing time and quality of results generated by our method and other methods for six 3D

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

W. Wang et al. / Saliency-Based Slicing Optimization

155

800
Number of layers

600

400

200

0

0.2

0.4

0.6

0.8

1

Random: The slicing result generated by randomly permuting the layers of the result generated by our method. Uniform: Uniform slicing result with the same layer thickness so that the printing time is almost the same as OurResult. DM94: Slicing result produced by the method in [DM94]. Sab96: Slicing result produced by the method in [Sab96]. HMS: Heuristic merging guided by our saliency metric. CEO: Geometric metric together with l0 optimization. HA13: Heuristic merging with geometric error deﬁned by the triangle area of the projected slicers in [HA13]. Given the same number of layers (i.e. the same printing time), we compare the visual quality of the Buddha model between OurResult and HA13 (see Figure 11). HA13 is a heuristic method and does not take into account the visual saliency of the model which is more sensitive to human eyes. Therefore, HA13 can only achieve suboptimal results. On the contrary, our method adaptively slices the 3D object with a visual saliency-based error metric. With a global l0 optimization, our method can achieve better slicing results. In Figure 11, the region bounded by the red rectangle is salient by human eyes, but the projection area of the adjacent layers is very small. Therefore, our method can slice this region with the ﬁnest resolution (see Figure 11b), while HA13 slices it with a particularly thick thickness (see Figure 11d).

Figure 9: The left side shows the relationship between the parameter ρ (x-axis) and the number of layers (y-axis) generated by SliceOpt (for the model in Figure 1 a). The right side shows the three objects printed with different ρ (which are 0.05, 0.2 and 0.5 from left to right).

models (see Figures 11, 12 and 14). Here are the descriptions of all methods:

GroundTruth: The ground truth slicing with the ﬁnest resolution α = 0.1 mm. OurResult: The slicing result produced by our AdapSlice algorithm. The segmentation algorithm is not executed before slice optimization for a fair comparison.

We then compare OurResult with HMS and CEO to prove that the l0 optimization cannot be replaced by a heuristic merging strategy and saliency metric cannot be replaced by another geometric metric (such as cusp height). Table 1 lists the relative SM difference with the same printing time and the printing time with the same relative SM difference of OurResult, HMS and CEO, respectively. Here, the relative SM difference (to GroundTruth) is deﬁned by RSMD(h) = E(h) ¯ , where E(h) is the SM difference as deﬁned in (3). Statistical E(h) results clearly show that our method is the best one. Figures 12(b), (g) and (h) show the visual effect of printed objects generated by OurResult, HMS and CEO, respectively. From the ﬁgure, we can see that the visual equality bounded by the red rectangle of Figures 12(g) and (h) have obvious ﬂaws, while Figure 12(b) is perfect. Although HMS uses the saliency metric, it merges the adjacent layers heuristically. Therefore, the slicing positions of their method are suboptimal and some layers may appear to be obvious ﬂaws, as in Figure 12(g). The cusp metric is used in CEO which focuses

800 750 700 650 600

Figure 10: Changes in the number of layers (left panel) and SM difference (right panel) during initialization and ﬁrst two iterations in SliceOpt (for the model in Figure 4 a).
c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

156

W. Wang et al. / Saliency-Based Slicing Optimization

Figure 11: Comparison of visual quality between our method (a) and [HA13] (c). Images (b) and (d) show the close-ups of the red marked regions.

Figure 12: Comparison of different slicing results for the Buddha head model. The ﬁrst row shows the close-up photos of the regions in red and the second row shows the photos of printed objects. the thick layers are located at the face. Our method can slice the face of the Buddha with the ﬁnest resolution according to the saliency metric and l0 optimization. That is why the visual artefacts shown in the insets of CEO are bigger than OurResult, as shown in Figure 12(h). In conclusion, our saliency metric and l0 optimization cannot be separated and replaced. Furthermore, we evaluate the visual quality of the results generated by different methods (OurResult, Random, Uniform, DM94 and Sab96) under the same printing time. The second to sixth columns of the Table 1 list the relative SM differences of the results generated by different methods. Note that the results generated by our method have the best visual quality except for GroundTruth. Figure 12 shows the photos of different slicing results. OurResult achieves the ﬁnest slicing layers at high salient regions while DM94 and Sab96 focus on regions with high curvature. Note that the results generated by our method preserve the details of the salient eye part of the head as the GroundTruth while the other methods produce coarser layers in the same region. In the close-up photos, visible artefacts can be seen in the results generated by Random, Uniform, DM94 and Sab96 methods. On the contrary, OurResult has no apparent difference from GroundTruth. As a result, the printed objects produced by our method have comparative visual quality with GroundTruth and are much better than the other methods.

Figure 13: Slicing results on a sphere model using different methods. (a) The result produced by the method from [DM94]; (b) The result produced by AdapSlice. on the regions with high curvature but not the high salient regions. Although CEO uses l0 optimization, it still slices the top of the Buddha with the ﬁnest resolution while slicing the face with a particularly thick thickness. l0 optimization can help CEO optimize the slicing positions and the number of layers, but it cannot slice the high salient regions with the ﬁnest possible resolution. Given the ﬁxed number of layers (i.e. the same printing time), the ﬁnest layers obtained by CEO are located at the top of head, while

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

W. Wang et al. / Saliency-Based Slicing Optimization

157

Table 1: Comparison of the relative SM difference under the same printing time (the left part of the table) and printing time under the same relative SM difference (the right part of the table) of different methods. OR, UT, C1 and C2 refer to the methods of OurResult, Uniform, DM94 and Sab96, respectively.

RSMD Model Buddha Rabbit Squirrel Laurana Lion CleanHead OR 0.0068 0.0075 0.0031 0.0038 0.0485 0.0028 UT 0.1250 0.1299 0.1255 0.1251 0.1252 0.1249 C1 0.1516 0.1083 0.1196 0.1520 0.1494 0.1948 C2 0.1556 0.1111 0.0971 0.0986 0.1333 0.1214 HMS 0.0781 0.0965 0.0802 0.0921 0.1102 0.0387 CEO 0.1487 0.1037 0.1145 0.1029 0.1265 0.1089 RSMD 0.1880 0.1411 0.0996 0.1093 0.1063 0.1276 OR 169 62 139 149 165 87 UT 207 88 178 198 199 115

Printing time (m) C1 232 92 175 202 201 132 C2 238 96 170 193 214 127 HMS 201 79 169 187 191 108 CEO 248 97 180 199 219 142

Figure 14: Photos of the printed objects using AdapSlice. The numbers in brackets below each photo denote its printing time (in minutes) and its ratio of time saving according to GroundTruth. As shown in the user study, there is no apparent visual difference between these objects and their corresponding ground truth. We also evaluate the printing time of the results that are generated by different methods (OurResult, Uniform, DM94 and Sab96) and have the same relative SM difference. The seventh column of the Table 1 lists the relative SM differences of different models. The last four columns of Table 1 list the printing time of OurResult, Uniform, DM94 and Sab96. Since DM94 and Sab96 slice the input model based on the cusp height metric only, they need to generate a large number of layers to preserve the visual quality of the printing result. On the contrary, our method adaptively slices the input model guided by the SM of the model. As a result, the printing time of OurResult is smallest compared to other results. More Results. Figure 13 shows an example of slicing a sphere model. As the saliency is constant on the sphere model, the SM (1) is approximately equal to the cusp measurement used in [DM94]. More slicing results generated by our algorithm for CAD models are shown in the Supporting Information. Figure 14 shows the photos of ﬁve printed objects whose slicing are generated using AdapSlice. Compared to the ﬁnest result, the results generated by our method take 26–34% less printing time. We also test AdapSlice on CAD models. Its performance is similar to the traditional adaptive slicing method [DM94]. User Study. We conduct a user study to further evaluate the visual quality of the results generated by our method. We choose six representative models from the data set. For each model, we generate six slicing results by applying the six methods mentioned above and manufacture them using the FDM printer. Thus we have six groups of printed objects, where each group consists of six printed objects for the same model (see details in the Supporting Information and the accompanying Video S1). Each subject is asked to select the top three objects with good visual quality from each group and rank the three objects. In order to make the study more reliable, we create another group (named JudgeG) of six printed objects, where each of them is created with a uniform layer thickness of (1 − λ)α + λα (λ ∈ [0, 1]). The ﬁrst subgroup of three objects are created with λ = 0.0, 0.1, 0.2, respectively, and the second subgroup of the other three objects were created with λ = 0.8, 0.9, 1.0, respectively. It is easy to determine that the objects in the ﬁrst subgroup are visually better than those in the second subgroup. If one subject chooses one of objects in the second subgroup within the top three visually acceptable objects in the user study, we regard this study invalid and discard the results.

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

158
1 0.8 0.6 0.4 0.2 0

W. Wang et al. / Saliency-Based Slicing Optimization

using data from our user study. Table 2 lists the user study data of rankings between various methods. For individual models, each row shows the number of subjects who rank OurResult preceding the results produced by other methods. Let p be the ratio of people (not limited to subjects in the user study) who consider OurResult is better than one other. For example, p = 1/2 means that two methods do not dominate each other. To answer such a question, we state the relevant null and alternative hypotheses as follows: H0 : p = 1/2; H1 : p = 1/2.

Figure 15: The results of user study. Here, the GT, OR, RE, UT, C1 and C2 are GroundTruth, OurResult, Random, Uniform, DM94 and Sab96, respectively.

Obviously, the number of successes for OurResult in competition to the results produced by some other method, denoted by X, is a random variable which follows binomial distribution B(n, p), where n = 64 is the total valid number of subjects in the user study. Given a signiﬁcance level α = 0.05, two numbers K1 and K2 can be calculated from following equations, respectively:
K1 −1 i=0 n i=K2 +1

A total of 67 subjects participate in the user study, comprised of 28 males and 39 females. The ages of the subjects are between 16 and 65. All subjects have normal or corrected-to-normal visual acuity. Because the objects are printed in white, we assume that the colour vision capability of the subjects has no effect on our study. Three of them are invalid and we have 64 valid user studies. Because these three subjects select at least one object in the second subgroup within the top three visually acceptable objects of JudgeG. The statistics of the user studies are shown in Figure 15. We make the following observations based on the results:

n i n i

i p0 (1 − p0 )n−i = α/2;

i p0 (1 − p0 )n−i = α/2.

r r

The objects produced by our method (OurResult) are superior to DM94 and Sab96 in most cases. The objects produced by our method (OurResult) are comparatively as good as those produced by GroundTruth: for three groups (Buddha, Laurana, CleanHead), OurResult are ranked as the best; for four groups (Buddha, Laurana, Lion, CleanHead), OurResult is ranked in the top two; for ﬁve groups (Buddha, Rabbit, Laurana, Lion, CleanHead), OurResult is ranked in the top three.

With p0 = 0.5, we get K1 = 25 and K2 = 38. Then we can use this region [K1 , K2 ] to determine what outcomes of a study would lead to a rejection of the null hypothesis for a pre-speciﬁed signiﬁcance level α = 0.05. If K1 ≤ X ≤ K2 , we accept the null hypothesis. If X > K2 (or X < K1 ), we would reject the null hypothesis in favour of the alternative, which means OurResult is statistically better (or worse) than the comparable one. From the user study data in Table 2, we come to the statistical inference that OurResult is comparable to GroundTruth on all models except Squirrel and dominates the results produced by other methods (Random, Uniform, DM94 and Sab96). Overall, we conclude that people can hardly distinguish the objects generated by our method with the corresponding ground truth objects from their appearance and our method is better than Random, Uniform, DM94 and Sab96 obviously, which means that our algorithm can save a large amount of printing time while preserving the visual salient regions of printed objects.
means ‘better than’ and ‘#’ means ‘the number’.

A result is considered statistically signiﬁcant if it has been predicted as unlikely to have occurred by chance alone, according to a pre-determined threshold probability, the signiﬁcance level α. In the following, we perform a statistical hypothesis test to draw inference

Table 2: The number of subjects who think OurResult is better than the results obtained by other methods. OR refer to our method.

Models Buddha Rabbit Squirrel Laurana Lion CleanHead

#(OR GroundTruth) 35 27 24 34 28 35

#(OR Uniform) 42 39 48 45 46 50

#(OR

Random) 46 41 39 47 41 52

#(OR DM94) 50 44 39 54 49 52

#(OR Sab96) 47 48 47 56 46 46

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

W. Wang et al. / Saliency-Based Slicing Optimization

159

Discussions and Limitations. Although we only test our method with an FDM printer, other types of printers like Selective Laser Sintering (SLS) and Stereolithography (SLA) printers follow the same assumption of our method and can also print layers with different printable thickness in similar time. In theory, our method can also be applied to save printing time for these printers. In practice, we did not validate our method on SLS and SLA printers because the source code of their slicing engines and machine code is not publicly available. Our segmentation algorithm saves printing time. However, it also produces slightly visual artefacts along the vertical boundaries. 8. Conclusions and Future Work In this paper, we present an efﬁcient method for slicing 3D models for the purpose of reducing printing time while preserving good visual salient regions as ground truth. Our adaptive layer thicknesses are generated according to visual saliency of the 3D model which means the regions with a high level of saliency should be sliced with high precision while the insigniﬁcant regions can be sliced with low precision. The best slicing orientation is determined based on total printing time, volumetric error and the printing stability before slicing optimization. To further reduce printing time, we propose a saliency-based segmentation method to partition the model into subparts and each subparts is sliced individually. By managing to modify the G-code of the FDM printer, we print the object as a whole. We have tested our algorithm on a variety of 3D complex models. Both the experimental results and the user study show that our method can efﬁciently reduce printing time while preserving good visual quality of produced objects as the ground truth. For an FDM printer, the printing time is inﬂuenced by many issues in the systematic manufacture technology like acceleration and deceleration of nozzle tip during material deposition, path planning, temperature, etc. For further speed up the printing process, further studies on these issues should be conducted. Acknowledgements We would like to thank the reviewers for their detailed comments and suggestions which greatly improved the manuscript. We are greatful to Lian et al. [LGB*11] and Shilane et al. [SMKF04] for their 3D databases. The work is supported by the One Hundred Talent Project of the Chinese Academy of Sciences and Natural Science Foundation of China (61222206, 61202284, 61173102, 61370143).

ulated models. ACM Transactions on Graphics 31, 6 (2012), 130:1–130:10. [CLD*13] CHEN D., LEVIN D. I., DIDYK P., SITTHI-AMORN P., MATUSIK W.: Spec2fab: A reducer-tuner model for translating speciﬁcations to 3D prints. ACM Transactions on Graphics 32 , 4 (2013), 135:1–135:10. [CLM*13] CEYLAN D., LI W., MITRA N., AGRAWALA M., PAULY M.: Designing and fabricating mechanical automata from mocap sequences. ACM Transactions on Graphics 32, 6 (2013), 186:1– 186:9. [CSPF12] CHEN X., SAPAROV A., PANG B., FUNKHOUSER T.: Schelling points on 3D surface meshes. ACM Transactions on Graphics 31, 4 (2012), 1–11. [CTN*13] COROS S., THOMASZEWSKI B., NORIS G., SUEDA S., FORBERG M., SUMNER R. W., MATUSIK W., BICKEL B.: Computational design of mechanical characters. ACM Transactions on Graphics 32, 4 (2013), 83:1–83:9. [Cur13] Cura: Website of the source code of cura. https:// github.com/daid/Cura (2013). Accessed October 2013. [CZM*11] CHENG M.-M., ZHANG G.-X., MITRA N., HUANG X., HU S.M.: Global contrast based salient region detection. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (Providence, RI, USA, 2011), pp. 409–416. ¨ A [DM94] DOLENC A., M¨ KELA I.: Slicing procedures for layered manufacturing techniques. Computer-Aided Design 26, 2 (1994), 119–126. [DSDB06] DIMITROV D., SCHREVE K., DE BEER N.: Advances in three dimensional printing-state of the art and future perspectives. Journal for New Generation Sciences 4, 1 (2006), 21–49. [HA13] HAYASI M. T., ASIABANPOUR B.: A new adaptive slicing approach for the fully dense freeform fabrication (FDFF) process. Journal of Intelligent Manufacturing 24, 4 (2013), 683–694. [HBA13] HILDEBRAND K., BICKEL B., ALEXA M.: Orthogonal slicing for additive manufacturing. Computers & Graphics 37, 6 (2013), 669–675. [HFW11] HAO J., FANG L., WILLIAMS R. E.: An efﬁcient curvaturebased partitioning of large-scale STL models. Rapid Prototyping Journal 17, 2 (2011), 116–127. [IKN98] ITTI L., KOCH C., NIEBUR E.: A model of saliency-based visual attention for rapid scene analysis. In IEEE Transactions on Pattern Analysis and Machine Intelligence 20 (1998), 1254– 1259. [JH95] JAMIESON R., HACKER H.: Direct slicing of CAD models for rapid prototyping. Rapid Prototyping Journal 1, 2 (1995), 4–12. [LBRM12] LUO L., BARAN I., RUSINKIEWICZ S., MATUSIK W.: Chopper: Partitioning models into 3D-printable parts. ACM Transactions on Graphics 31, 6 (2012), 129:1–129:9.

References [AAD98] ALEXANDER P., ALLEN S., DUTTA D.: Part orientation and build cost determination in layered manufacturing. ComputerAided Design 30, 5 (1998), 343–356.
A [BBJP12] B¨ CHER M., BICKEL B., JAMES D. L., PFISTER H.: Fabricating articulated characters from skinned meshes. ACM Transactions on Graphics 31, 4 (2012), 47:1–47:9. I [CCA*12] CAL` J., CALIAN D. A., AMATI C., KLEINBERGER R., STEED A., KAUTZ J., WEYRICH T.: 3D-printing of non-assembly, artic-

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

160

W. Wang et al. / Saliency-Based Slicing Optimization

[LDPT13] LAN Y., DONG Y., PELLACINI F., TONG X.: Bi-scale appearance fabrication. ACM Transactions on Graphics 32, 4 (2013), 145:1–145:12. [LGB*11] LIAN Z., GODIL A., BUSTOS B., DAOUDI M., HERMANS J., KAWAMURA S., VANDERMEULEN D.: SHREC’11 track: Shape retrieval on non-rigid 3D watertight meshes. In Proceedings of the Eurographics Workshop on 3D Object Retrieval (3DOR’11) (Switzerland, 2011), pp. 79–88. [LGX*13] LEVIN A., GLASNER D., XIONG Y., DURAND F., FREEMAN W., MATUSIK W., ZICKLER T.: Fabricating BRDFs at high spatial resolution using wave optics. ACM Transactions on Graphics 32, 4 (2013), Article No. 144. [LVJ05] LEE C. H., VARSHNEY A., JACOBS D. W.: Mesh saliency. ACM Transactions on Graphics 24 (2005), 659–666. [Mak12] Makerbot: Rapid prototyping and 3D printing. http://www.makerbot.com/ (2012). November 2012. [MKD99] MANI K., KULKARNI P., DUTTA D.: Region-based adaptive slicing. Computer-Aided Design 31, 5 (1999), 317–333. [NW06] NOCEDAL J., WRIGHT S.: Numerical Optimization (2nd edition). Springer, New York, 2006. [PRD03] PANDEY P. M., REDDY N. V., DHANDE S. G.: Slicing procedures in layered manufacturing: A review. Rapid Prototyping Journal 9, 5 (2003), 274–288. ´ [PWLSH13] PREVOST R., WHITING E., LEFEBVRE S., SORKINEHORNUNG O.: Make it stand: Balancing shapes for 3D fabrication. ACM Transactions on Graphics 32, 4 (2013), 81:1–81:10. [Sab96] SABOURIN E.: Adaptive High-Precision Exterior, HighSpeed Interior, Layered Manufacturing. PhD thesis, Virginia Polytechnic Institute and State University, 1996. [SLM*14] SONG R., LIU Y., MARTIN R., ROSIN P., et al.: Mesh saliency via spectral processing. ACM Transactions on Graphics 31, 1 (2014), 6:1–6:17. [SMKF04] SHILANE P., MIN P., KAZHDAN M., FUNKHOUSER T.: The princeton shape benchmark. In Proceedings of Shape Modeling Application (Palazzo Ducale, Genova, Italy, 2004), pp. 167–178.
E [SVB*12] STAVA O., VANEK J., BENES B., CARR N., Mˇ CH R.: Stress relief: Improving structural strength of 3D printable objects. ACM Transactions on Graphics 31, 4 (2012), 48:1–48:11.

[VGB*14] VANEK J., GALICIA J., BENES B., Mˇ ch R., CARR N., STAVA e O., MILLER G.: Packmerger: A 3D print volume optimizer. Computer Graphics Forum 33, 6 (2014), 322–332. ˇ [VWRKM13] VIDIMCE K., WANG S.-P., RAGAN-KELLEY J., MATUSIK W.: Openfab: A programmable pipeline for multi-material fabrication. ACM Transactions on Graphics 32, 4 (2013), 136:1– 136:10. [WSZL13] WU J., SHEN X., ZHU W., LIU L.: Mesh saliency with global rarity. Graphical Models 75, 5 (2013), 255– 264. [WWY*13] WANG W., WANG T. Y., YANG Z., LIU L., TONG X., TONG W., DENG J., CHEN F., LIU X.: Cost-effective printing of 3D objects with skin-frame structures. ACM Transactions on Graphics 32, 5 (2013), 177:1–177:10. [XWL*97] XU F., WONG Y., LOH H., FUH J., MIYAZAWA T.: Optimal orientation with variable slicing in stereolithography. Rapid Prototyping Journal 3, 3 (1997), 76–88. [ZC95] ZHU P., CHIRLIAN P.: On critical point detection of digital shapes. IEEE Transactions on Pattern Recognition and Machine Intelligence 17, 8 (1995), 737–748. [ZPZ13] ZHOU Q., PANETTA J., ZORIN D.: Worst-case structural analysis. ACM Transactions on Graphics 32, 4 (2013), 137:1– 137:12. [ZXS*12] ZHU L., XU W., SNYDER J., LIU Y., WANG G., GUO B.: Motion-guided mechanical toy modeling. ACM Transactions on Graphics 31, 6 (2012), 127:1–127:9. Supporting Information Additional Supporting Information may be found in the online version of this article at the publisher’s web site: Figure S1: The collection of models used in our test. Figure S2: Three groups of objects used in our user study and their zoomed-in images. Figure S3: Another three groups of the objects used in our user study and their zoomed-in images. Figure S4: The seventh group used to judge believability of the answers of the participants. Figure S5: More slicing results generated by our method for CAD models. Video S1.

[TB98] TYBERG J., BØHN J. H.: Local adaptive slicing. Rapid Prototyping Journal 4, 3 (1998), 118–127.

c 2015 The Authors Computer Graphics Forum c 2015 The Eurographics Association and John Wiley & Sons Ltd.

